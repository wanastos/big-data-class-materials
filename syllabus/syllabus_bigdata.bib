@misc{mcdermott2023parallel,
   author = {Grant McDermott and Ed Rubin},
   journal = {Data Science for Economists and Other Animals},
   title = {Parallel programming},
   url = {https://grantmcdermott.com/ds4e/parallel.html},
   year = {2023},
}
@misc{mcdermott2023spatial,
   author = {Grant McDermott and Ed Rubin},
   journal = {Data Science for Economists and Other Animals},
   title = {Spatial analysis},
   url = {https://grantmcdermott.com/ds4e/spatial-analysis.html},
   year = {2023},
}
@misc{mcdermott2023tidy,
   author = {Grant McDermott},
   journal = {Data Science for Economists and Other Animals},
   title = {Tidyverse},
   url = {https://grantmcdermott.com/ds4e/tidyverse.html},
   year = {2023},
}
@article{eddelbuettel2021parallel,
   abstract = {Parallel computing has established itself as another standard method for applied research and data analysis. The R system, being internally constrained to mostly singly-threaded operations, can nevertheless be used along with different parallel computing approaches. This brief review covers OpenMP and Intel TBB at the CPU- and compiler level, moves to process-parallel approaches before discussing message-passing parallelism and big data technologies for parallel processing such as Spark, Docker and Kubernetes before concluding with a focus on the future package integrating many of these approaches. This article is categorized under: Algorithms and Computational Methods > Methods for High Performance Computing Software for Computational Statistics > Software/Statistical Software Software for Computational Statistics > High Performance Software.},
   author = {Dirk Eddelbuettel},
   doi = {10.1002/WICS.1515},
   issn = {19390068},
   issue = {2},
   journal = {Wiley Interdisciplinary Reviews: Computational Statistics},
   keywords = {Kubernetes,OpenMP,OpenMPI,R,parallel computing,r,spark},
   month = {3},
   publisher = {Wiley-Blackwell},
   title = {Parallel computing with R: A brief review},
   volume = {13},
   year = {2021},
}
@misc{mcdermott2023functionsadvanced,
   author = {Grant McDermott and Ed Rubin},
   journal = {Data Science for Economists and Other Animals},
   title = {Functions: Advanced concepts},
   url = {https://grantmcdermott.com/ds4e/funcs-adv.html},
   year = {2023},
}
@misc{mcdermott2023functionsintro,
   author = {Grant McDermott and Ed Rubin},
   journal = {Data Science for Economists and Other Animals},
   title = {Functions: Introductory concepts},
   url = {https://grantmcdermott.com/ds4e/funcs-intro.html},
   year = {2023},
}
@article{ginsberg2009influenza,
   abstract = {This paper - first published on-line in November 2008 - draws on data from an early version of the Google Flu Trends search engine to estimate the levels of flu in a population. It introduces a computational model that converts raw search query data into a region-by-region real-time surveillance system that accurately estimates influenza activity with a lag of about one day - one to two weeks faster than the conventional reports published by the Centers for Disease Prevention and Control. This report introduces a computational model based on internet search queries for real-time surveillance of influenza-like illness (ILI), which reproduces the patterns observed in ILI data from the Centers for Disease Control and Prevention. Seasonal influenza epidemics are a major public health concern, causing tens of millions of respiratory illnesses and 250,000 to 500,000 deaths worldwide each year1. In addition to seasonal influenza, a new strain of influenza virus against which no previous immunity exists and that demonstrates human-to-human transmission could result in a pandemic with millions of fatalities2. Early detection of disease activity, when followed by a rapid response, can reduce the impact of both seasonal and pandemic influenza3,4. One way to improve early detection is to monitor health-seeking behaviour in the form of queries to online search engines, which are submitted by millions of users around the world each day. Here we present a method of analysing large numbers of Google search queries to track influenza-like illness in a population. Because the relative frequency of certain queries is highly correlated with the percentage of physician visits in which a patient presents with influenza-like symptoms, we can accurately estimate the current level of weekly influenza activity in each region of the United States, with a reporting lag of about one day. This approach may make it possible to use search queries to detect influenza epidemics in areas with a large population of web search users.},
   author = {Jeremy Ginsberg and Matthew H. Mohebbi and Rajan S. Patel and Lynnette Brammer and Mark S. Smolinski and Larry Brilliant},
   doi = {10.1038/nature07634},
   issn = {1476-4687},
   issue = {7232},
   journal = {Nature 2009 457:7232},
   keywords = {Humanities and Social Sciences,Science,multidisciplinary},
   month = {2},
   pages = {1012-1014},
   pmid = {19020500},
   publisher = {Nature Publishing Group},
   title = {Detecting influenza epidemics using search engine query data},
   volume = {457},
   url = {https://www.nature.com/articles/nature07634},
   year = {2009},
}
@article{stephensdavidowitz2014racial,
   abstract = {How can we know how much racial animus costs a black presidential candidate, if many people lie to surveys? I suggest a new proxy for an area's racial animus from a non-survey source: the percent of Google search queries that include racially charged language. I compare the proxy to Barack Obama's vote shares, controlling for the vote share of the previous Democratic presidential candidate, John Kerry. An area's racially charged search rate is a robust negative predictor of Obama's vote share. Continuing racial animus in the United States appears to have cost Obama roughly four percentage points of the national popular vote in both 2008 and 2012. The estimates using Google search data are 1.5 to 3 times larger than survey-based estimates. © 2014 Elsevier B.V.},
   author = {Seth Stephens-Davidowitz},
   doi = {10.1016/J.JPUBECO.2014.04.010},
   issn = {0047-2727},
   journal = {Journal of Public Economics},
   keywords = {Discrimination,Google,Voting},
   month = {10},
   pages = {26-40},
   publisher = {North-Holland},
   title = {The cost of racial animus on a black candidate: Evidence using Google search data},
   volume = {118},
   year = {2014},
}
@book{wickham2023meta,
   author = {Hadley Wickham},
   journal = {Advanced R},
   title = {Metaprogramming},
   url = {https://adv-r.hadley.nz/metaprogramming.html},
   year = {2023},
}
@misc{tidyeval,
   title = {Tidy eval helpers — tidyeval • ggplot2},
   url = {https://ggplot2.tidyverse.org/reference/tidyeval.html},
}
@article{eddelbuettel2020parallel,
   abstract = {Parallel computing has established itself as another standard method for applied research and data analysis. The R system, being internally constrained to mostly singly-threaded operations, can nevertheless be used along with different parallel computing approaches. This brief review covers OpenMP and Intel TBB at the CPU-and compiler level, moves to process-parallel approaches before discussing message-passing parallelism and big data technologies for parallel processing such as Spark, Docker and Kubernetes before concluding with a focus on the future package integrating many of these approaches.},
   author = {Dirk Eddelbuettel},
   keywords = {Kubernetes,OpenMP,OpenMPI,Parallel Computing,R,Spark},
   title = {Parallel Computing With R: A Brief Review},
   url = {https://arxiv.org/abs/1912.11144},
   year = {2020},
}
@article{zimmerman2014returns,
   abstract = {I combine a regression discontinuity design with rich data on academic and labor market outcomes for a large sample of Florida students to estimate the returns to college admission for academically marginal students. Students with grades just above a threshold for admissions eligibility at a large public university in Florida are much more likely to attend any university than below-threshold students. The marginal admission yields earnings gains of 22% between 8 and 14 years after high school completion. These gains outstrip the costs of college attendance, and they are largest for male students and free-lunch recipients.},
   author = {Seth D. Zimmerman},
   doi = {10.1086/676661},
   issn = {0734306X},
   issue = {4},
   journal = {Journal of Labor Economics},
   month = {10},
   pages = {711-754},
   publisher = {University of Chicago Press},
   title = {The returns to college admission for academically marginal students},
   volume = {32},
   url = {https://www.journals.uchicago.edu/doi/full/10.1086/676661},
   year = {2014},
}
@article{dell2010mita,
   abstract = {This study utilizes regression discontinuity to examine the long-run impacts of the mita, an extensive forced mining labor system in effect in Peru and Bolivia between 1573 and 1812. Results indicate that a mita effect lowers household consumption by around 25% and increases the prevalence of stunted growth in children by around 6 percentage points in subjected districts today. Using data from the Spanish Empire and Peruvian Republic to trace channels of institutional persistence, I show that the mita's influence has persisted through its impacts on land tenure and public goods provision. Mita districts historically had fewer large landowners and lower educational attainment. Today, they are less integrated into road networks and their residents are substantially more likely to be subsistence farmers.},
   author = {Melissa Dell},
   doi = {10.3982/ECTA8121},
   issue = {6},
   journal = {Econometrica},
   keywords = {Forced labor,land tenure,public goods},
   pages = {1863-1903},
   title = {The Persistent Effects of Peru's Mining Mita},
   volume = {78},
   url = {https://scholar.harvard.edu/files/dell/files/ecta8121_0.pdf},
   year = {2010},
}
@misc{simonsohn2023bertrand,
   author = {Uri Simonsohn},
   title = {[51] Greg vs. Jamal: Why Didn’t Bertrand and Mullainathan (2004) Replicate? - Data Colada},
   url = {https://datacolada.org/51},
}
@article{mullainathan2017machine,
   author = {Sendhil Mullainathan and Jann Spiess},
   doi = {10.1257/jep.31.2.87},
   issue = {2},
   journal = {Journal of Economic Perspectives},
   pages = {87-106},
   title = {Machine Learning: An Applied Econometric Approach},
   volume = {31},
   url = {https://doi.org/10.1257/jep.31.2.87},
   year = {2017},
}
@article{kleinberg2015prediction,
   author = {Jon Kleinberg and Jens Ludwig and Sendhil Mullainathan and Ziad Obermeyer},
   doi = {10.1257/AER.P20151023},
   issn = {0002-8282},
   issue = {5},
   journal = {American Economic Review},
   keywords = {Belief,Communication,Econometric Modeling: General, Forecasting Models,Information and Knowledge,Learning,Simulation Methods, Search,Unawareness},
   month = {5},
   pages = {491-95},
   publisher = {American Economic Association},
   title = {Prediction Policy Problems},
   volume = {105},
   year = {2015},
}
@article{Chetty2020,
   abstract = {We study the sources of racial disparities in income using anonymized longitudinal data covering nearly the entire U.S. population from 1989 to 2015. We document three results. First, black Americans and American Indians have much lower rates of upward mobility and higher rates of downwardmobility than whites, leading to persistent disparities across generations. Conditional on parent income, the black-white income gap is driven by differences inwages and employment rates between black and white men; there are no such differences between black and white women. Hispanic Americans have rates of intergenerational mobility more similar to whites than blacks, leading the Hispanic-white income gap to shrink across generations. Second, differences in parental marital status, education, and wealth explain little of the black-white income gap conditional on parent income. Third, the black-white gap persists even among boys who grow up in the same neighborhood. Controlling for parental income, black boys have lower incomes in adulthood than white boys in 99% of Census tracts. The few areas with small black-white gaps tend to be low-poverty neighborhoods with low levels of racial bias among whites and high rates of father presence among blacks. Black males who move to such neighborhoods earlier in childhood have significantly better outcomes. However, less than 5% of black children grow up in such areas. Our findings suggest that reducing the black-white income gap will require efforts whose impacts cross neighborhood and class lines and increase upward mobility specifically for black men. JEL Code: J0.},
   author = {Raj Chetty and Nathaniel Hendren and Maggie R Jones and Sonya R Porter and Rick Banks and Jennifer Eberhardt and David Ellwood and John Friedman and Roland Fryer and David Grusky and Anthony Jack and Lawrence Katz and Adam Looney and Alex Mas and Amy O'hara and Amanda Pallais and John Powell and Sean Reardon and Jesse Shapiro and Emmanuel Saez and Robert Sampson and Isaac Sorkin and Floren-Cia Torche and William Julius Wilson and Michael Droste and Benjamin Goldman and Jamie Gracie and Alexandre Jenni and Martin Koenen and Sarah Merchant and Emanuel Schertz and James Stratton and Joseph Winkelmann},
   doi = {10.1093/QJE/QJZ042},
   issn = {0033-5533},
   issue = {2},
   journal = {The Quarterly Journal of Economics},
   month = {5},
   pages = {711-783},
   publisher = {Oxford Academic},
   title = {Race and Economic Opportunity in the United States: an Intergenerational Perspective},
   volume = {135},
   url = {https://dx.doi.org/10.1093/qje/qjz042},
   year = {2020},
}
@article{card1993college,
   author = {David Card},
   city = {Cambridge, MA},
   doi = {10.3386/W4483},
   institution = {National Bureau of Economic Research},
   keywords = {David Card},
   month = {10},
   title = {Using Geographic Variation in College Proximity to Estimate the Return to Schooling},
   url = {https://www.nber.org/papers/w4483},
   year = {1993},
}
@misc{regexplain,
   title = {Garrick Aden-Buie - 🕵️‍♂️ RegExplain},
   url = {https://www.garrickadenbuie.com/project/regexplain/},
}
@article{crs,
   abstract = {Coordinate reference systems CRS provide a standardized way of describing locations. Many different CRS are used to describe geographic data. The CRS that is chosen depends on when the data was collected, the geographic extent of the data, the purpose of the data, etc. In R, when data with different CRS are combined it is important to transform them to a common CRS so they align with one another. This is similar to making sure that units are the same when measuring volume or distances. Package sp and rgdal is used to assign and transform CRS in R: library(rgdal) library(sp) In R, the notation used to describe the CRS is proj4string from the PROJ.4 library. It looks like this: +init=epsg:4121 +proj=longlat +ellps=GRS80 +datum=GGRS87 +no_defs +towgs84=-199.87,74.79,246.62 There are various attributes of the CRS, such as the projection, datum, and ellipsoid. Some of the options for each variable can be obtained in R with projInfo: 1. Projection: projInfo(type = "proj") 2. Datum: projInfo(type = "datum") 3. Ellipsoid: projInfo(type = "ellps")},
   title = {Overview of Coordinate Reference Systems (CRS) in R},
   url = {http://spatialreference.org/},
}
@misc{selectorgadget,
   title = {SelectorGadget • rvest},
   url = {https://rvest.tidyverse.org/articles/selectorgadget.html},
}
@misc{apiintro,
   title = {An introduction to APIs | Zapier guides},
   url = {https://zapier.com/resources/guides/apis},
}
@misc{tinyverse,
   title = {tinyverse},
   url = {https://www.tinyverse.org/},
}
@article{wickhamtidy,
   abstract = {A huge amount of effort is spent cleaning data to get it ready for analysis, but there has been little research on how to make data cleaning as easy and effective as possible. This paper tackles a small, but important, component of data cleaning: data tidying. Tidy datasets are easy to manipulate, model and visualise, and have a specific structure: each variable is a column, each observation is a row, and each type of observational unit is a table. This framework makes it easy to tidy messy datasets because only a small set of tools are needed to deal with a wide range of untidy datasets. This structure also makes it easier to develop tidy tools for data analysis, tools that both input and output tidy datasets. The advantages of a consistent data structure and matching tools are demonstrated with a case study free from mundane data manipulation chores.},
   author = {Hadley Wickham},
   keywords = {R,data cleaning,data tidying,relational databases},
   title = {Journal of Statistical Software Tidy Data},
   url = {http://www.jstatsoft.org/},
}
@misc{tidy,
   title = {Tidy data • tidyr},
   url = {https://tidyr.tidyverse.org/articles/tidy-data.html},
}
@misc{rmarkdownyihui,
   title = {R Markdown: The Definitive Guide},
   url = {https://bookdown.org/yihui/rmarkdown/},
}
@misc{rmarkdown,
   title = {R Markdown},
   url = {https://rmarkdown.rstudio.com/},
}
@book{lovelace2019geographic,
   author = {Robin Lovelace and Jakub Nowosad and Jannes Muenchow},
   isbn = {9780203730058},
   issue = {March},
   title = {Chapter 2 Geographic data in R | Geocomputation with R},
   url = {https://r.geocompx.org/spatial-class.html},
   year = {2019},
}
@article{song2023inflation,
   abstract = {We study the relationship between media portrayals of inflation and consumer sentiment. Using tools from natural language processing, we uncover two competing narratives in US news coverage of inflation: the first relates inflation to financial variables, while the second relates inflation to real variables. As inflation rose in 2021, media increasingly emphasized the real economy. Linking inflation news to social network data from Twitter, we find that exposure to articles emphasizing the connection between inflation and the real economy significantly reduces sentiment, particularly in periods of high inflation. Shifting media narratives may therefore have contributed to declining consumer sentiment in 2021.},
   author = {Alistair Macaulay and Wenting Song},
   doi = {10.1257/PANDP.20231117},
   issn = {2574-0768},
   journal = {AEA Papers and Proceedings},
   keywords = {Belief,Communication,Deflation, Entertainment,Inflation,Information and Knowledge,Learning,Media,Neural Networks and Related Topics, Search,Unawareness, Price Level},
   month = {5},
   pages = {172-76},
   publisher = {American Economic Association},
   title = {News Media, Inflation, and Sentiment},
   volume = {113},
   year = {2023},
}
@article{moore2019temp,
   abstract = {The changing global climate is producing increasingly unusual weather relative to preindustrial conditions. In an absolute sense, these changing conditions constitute direct evidence of anthropogenic climate change. However, human evaluation of weather as either normal or abnormal will also be influenced by a range of factors including expectations, memory limitations, and cognitive biases. Here we show that experience of weather in recent years- rather than longer historical periods-determines the climatic baseline against which current weather is evaluated, potentially obscuring public recognition of anthropogenic climate change. We employ variation in decadal trends in temperature at weekly and county resolution over the continental United States, combined with discussion of the weather drawn from over 2 billion social media posts. These data indicate that the remarkability of particular temperatures changes rapidly with repeated exposure. Using sentiment analysis tools, we provide evidence for a "boiling frog" effect: The declining noteworthiness of historically extreme temperatures is not accompanied by a decline in the negative sentiment that they induce, indicating that social normalization of extreme conditions rather than adaptation is driving these results. Using climate model projections we show that, despite large increases in absolute temperature, anomalies relative to our empirically estimated shifting baseline are small and not clearly distinguishable from zero throughout the 21st century.},
   author = {Frances C. Moore and Nick Obradovich and Flavio Lehner and Patrick Baylis},
   doi = {10.1073/PNAS.1816541116/-/DCSUPPLEMENTAL},
   issn = {10916490},
   issue = {11},
   journal = {Proceedings of the National Academy of Sciences of the United States of America},
   keywords = {Baseline,Climate change,Perception,Temperature,Twitter},
   pages = {4905-4910},
   pmid = {30804179},
   publisher = {National Academy of Sciences},
   title = {Rapidly declining remarkability of temperature anomalies may obscure public perception of climate change},
   volume = {116},
   year = {2019},
}
@book{cunningham2023mixtape,
   author = {Scott Cunningham},
   title = {Causal Inference: The Mixtape},
   volume = {Yale Press.},
   url = {https://mixtape.scunning.com/04-potential_outcomes},
   year = {2021},
}
@book{hungtintonklein2023effect,
   author = {Nick Huntington-Klein},
   title = {The Effect: An introduction to research and causality},
   volume = {Chapman and Hall},
   url = {https://theeffectbook.net/ch-StatisticalAdjustment.html},
   year = {2021},
}
@misc{chetty2019moving,
   author = {Peter Bergman and Raj Chetty and Stefanie, Deluca and Nathaniel Hendren and Lawrence F. Katz and Christopher Palmer},
   city = {Cambridge, MA},
   doi = {10.3386/W26164},
   institution = {National Bureau of Economic Research},
   keywords = {Christopher Palmer,Lawrence F. Katz,Nathaniel Hendren,Peter Bergman,Raj Chetty,Stefanie DeLuca},
   month = {8},
   title = {Creating Moves to Opportunity: Experimental Evidence on Barriers to Neighborhood Choice},
   url = {https://www.nber.org/papers/w26164},
   year = {2019},
}
@misc{mcdermott2022docker,
   author = {Grant McDermott},
   journal = {Data Science for Economists},
   title = {Docker Lecture},
   url = {https://raw.githack.com/uo-ec607/lectures/master/13-docker/13-docker.html#1},
   year = {2022},
}
@misc{datacolada2022groundhog,
   author = {Uri Simonsohn},
   journal = {Data Colada},
   title = {[100] Groundhog 2.0: Further addressing the threat R poses to reproducible research - Data Colada},
   url = {http://datacolada.org/100},
   year = {2022},
}
@misc{datacolada2021groundhog,
   author = {Uri Simonsohn},
   journal = {Data Colada},
   title = {[95] Groundhog: Addressing The Threat That R Poses To Reproducible Research - Data Colada},
   url = {https://datacolada.org/95},
   year = {2021},
}
@article{cavallo2016billion,
   author = {Alberto Cavallo and Roberto Rigobon},
   doi = {10.1257/JEP.30.2.151},
   issn = {0895-3309},
   issue = {2},
   journal = {Journal of Economic Perspectives},
   keywords = {Belief,Communication,Deflation,Inflation,Information and Knowledge,Large Data Sets: Modeling and Analysis, Search,Learning,Unawareness, Price Level},
   month = {3},
   pages = {151-78},
   publisher = {American Economic Association},
   title = {The Billion Prices Project: Using Online Prices for Measurement and Research},
   volume = {30},
   year = {2016},
}
@article{lazer2014parable,
   abstract = {Large errors in flu prediction were largely avoidable, which offers lessons for the use of big data.},
   author = {David Lazer and Ryan Kennedy and Gary King and Alessandro Vespignani},
   doi = {10.1126/SCIENCE.1248506/SUPPL_FILE/1248506.LAZER.SM.REVISION1.PDF},
   issn = {10959203},
   issue = {6176},
   journal = {Science},
   month = {3},
   pages = {1203-1205},
   pmid = {24626916},
   publisher = {American Association for the Advancement of Science},
   title = {The parable of google flu: Traps in big data analysis},
   volume = {343},
   url = {https://www.science.org/doi/10.1126/science.1248506},
   year = {2014},
}
@article{angrist1999maimonides,
   abstract = {The twelfth century rabbinic scholar Maimonides proposed a maximum class size of 40. This same maximum induces a nonlinear and nonmonotonic relationship between grade enrollment and class size in Israeli public schools today. Maimonides' rule of 40 is used here to construct instrumental variables estimates of effects of class size on test scores. The resulting identification strategy can be viewed as an application of Donald Campbell's regression-discontinuity design to the class-size question. The estimates show that reducing class size induces a significant and substantial increase in test scores for fourth and fifth graders, although not for third graders.},
   author = {Joshua D. Angrist and Victor Lavy},
   doi = {10.1162/003355399556061},
   issn = {0033-5533},
   issue = {2},
   journal = {The Quarterly Journal of Economics},
   month = {5},
   pages = {533-575},
   publisher = {Oxford Academic},
   title = {Using Maimonides' Rule to Estimate the Effect of Class Size on Scholastic Achievement},
   volume = {114},
   url = {https://dx.doi.org/10.1162/003355399556061},
   year = {1999},
}
@article{chetty2011kindergarten,
   abstract = {In Project STAR, 11,571 students in Tennessee and their teachers were randomly assigned to classrooms within their schools from kindergarten to third grade. This article evaluates the long-term impacts of STAR by linking the experimental data to administrative records. We first demonstrate that kindergarten test scores are highly correlated with outcomes such as earnings at age 27, college attendance, home ownership, and retirement savings. We then document four sets of experimental impacts. First, students in small classes are significantly more likely to attend college and exhibit improvements on other outcomes. Class size does not have a significant effect on earnings at age 27, but this effect is imprecisely estimated. Second, students who had a more experienced teacher in kindergarten have higher earnings. Third, an analysis of variance reveals significant classroom effects on earnings. Students who were randomly assigned to higher quality classrooms in grades K-3-as measured by classmates' end-of-class test scores-have higher earnings, college attendance rates, and other outcomes. Finally, the effects of class quality fade out on test scores in later grades, but gains in noncognitive measures persist. © The Author(s) 2011. Published by Oxford University Press, on behalf of President and Fellows of Harvard College. All rights reserved.},
   author = {Raj Chetty and John N. Friedman and Nathaniel Hilger and Emmanuel Saez and Diane Whitmore Schanzenbach and Danny Yagan},
   doi = {10.1093/QJE/QJR041},
   issn = {0033-5533},
   issue = {4},
   journal = {The Quarterly Journal of Economics},
   month = {11},
   pages = {1593-1660},
   pmid = {22256342},
   publisher = {Oxford Academic},
   title = {How Does Your Kindergarten Classroom Affect Your Earnings? Evidence from Project Star},
   volume = {126},
   url = {https://dx.doi.org/10.1093/qje/qjr041},
   year = {2011},
}
@article{dwork2006differential,
   abstract = {In 1977 Dalenius articulated a desideratum for statistical databases: nothing about an individual should be learnable from the database that cannot be learned without access to the database. We give a general impossibility result showing that a formalization of Dalenius' goal along the lines of semantic security cannot be achieved. Contrary to intuition, a variant of the result threatens the privacy even of someone not in the database. This state of affairs suggests a new measure, differential privacy, which, intuitively, captures the increased risk to one's privacy incurred by participating in a database. The techniques developed in a sequence of papers [8, 13, 3], culminating in those described in [12], can achieve any desired level of privacy under this measure. In many cases, extremely accurate information about the database can be provided while simultaneously ensuring very high levels of privacy. © Springer-Verlag Berlin Heidelberg 2006.},
   author = {Cynthia Dwork},
   doi = {10.1007/11787006_1/COVER},
   isbn = {3540359079},
   issn = {16113349},
   journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
   pages = {1-12},
   publisher = {Springer Verlag},
   title = {Differential privacy},
   volume = {4052 LNCS},
   url = {https://link.springer.com/chapter/10.1007/11787006_1},
   year = {2006},
}
@article{abowd2019privacy,
   abstract = {Statistical agencies face a dual mandate to publish accurate statistics while protecting respondent privacy. Increasing privacy protection requires decreased accuracy. Recognizing this as a resource allocation problem, we propose an economic solution: operate where the marginal cost of increasing privacy equals the marginal benefit. Our model of production, from computer science, assumes data are published using an efficient differentially private algorithm. Optimal choice weighs the demand for accurate statistics against the demand for privacy. Examples from US statistical programs show how our framework can guide decision-making. Further progress requires a better understanding of willingness-to-pay for privacy and statistical accuracy. (JEL C38, C81, D83)},
   author = {John M. Abowd and Ian M. Schmutte},
   doi = {10.1257/AER.20170627},
   issn = {0002-8282},
   issue = {1},
   journal = {American Economic Review},
   keywords = {Belief,Cluster Analysis,Communication,Data Access, Search,Factor Models, Methodology for Collecting, Estimating, and Organizing Microeconomic Data,Information and Knowledge,Learning,Multiple or Simultaneous Equation Models: Classification Methods,Principal Components,Unawareness},
   month = {1},
   pages = {171-202},
   publisher = {American Economic Association},
   title = {An Economic Analysis of Privacy Protection and Statistical Accuracy as Social Choices},
   volume = {109},
   year = {2019},
}
@misc{crespo2021remittances,
   author = {Jesús Crespo Cuaresma and Max Thomasberger},
   journal = {Brookings},
   title = {Modeling remittances flows with web scraping},
   url = {https://www.brookings.edu/articles/modeling-remittances-flows-with-web-scraping/},
   year = {2021},
}
@article{chetty2016moving,
   abstract = {The Moving to Opportunity (MTO) experiment offered randomly selected families housing vouchers to move from high-poverty housing projects to lower-poverty neighborhoods. We analyze MTO's impacts on children's long-term outcomes using tax data. We find that moving to a lower-poverty neighborhood when young (before age 13) increases college attendance and earnings and reduces single parenthood rates. Moving as an adolescent has slightly negative impacts, perhaps because of disruption effects. The decline in the gains from moving with the age when children move suggests that the duration of exposure to better environments during childhood is an important determinant of children's long-term outcomes. (JEL I31, I38, J13, R23, R38).},
   author = {Raj Chetty and Nathaniel Hendren and Lawrence F. Katz},
   doi = {10.1257/AER.20150572},
   issn = {0002-8282},
   issue = {4},
   journal = {American Economic Review},
   keywords = {Child Care,Children,Family Planning,General Welfare,Neighborhood Characteristics, Production Analysis and Firm Location: Government Policy,Population,Provision and Effects of Welfare Programs, Fertility,Regional Labor Markets,Well-Being, Welfare, Well-Being, and Poverty: Government Programs,Youth, Urban, Rural, Regional, Real Estate, and Transportation Economics: Regional Migration},
   month = {4},
   pages = {855-902},
   pmid = {29546974},
   publisher = {American Economic Association},
   title = {The Effects of Exposure to Better Neighborhoods on Children: New Evidence from the Moving to Opportunity Experiment},
   volume = {106},
   year = {2016},
}
@article{kleinberg2018human,
   abstract = {Can machine learning improve human decision making? Bail decisions provide a good test case. Millions of times each year, judges make jail-or-release decisions that hinge on a prediction of what a defendant would do if released. The concreteness of the prediction task combined with the volume of data available makes this a promising machine-learning application. Yet comparing the algorithm to judges proves complicated. First, the available data are generated by prior judge decisions. We only observe crime outcomes for released defendants, not for those judges detained. This makes it hard to evaluate counterfactual decision rules based on algorithmic predictions. Second, judges may have a broader set of preferences than the variable the algorithm predicts; for instance, judges may care specifically about violent crimes or about racial inequities. We deal with these problems using different econometric strategies, such as quasi-random assignment of cases to judges. Even accounting for these concerns, our results suggest potentially large welfare gains: one policy simulation shows crime reductions up to 24.7% with no change in jailing rates, or jailing rate reductions up to 41.9% with no increase in crime rates. Moreover, all categories of crime, including violent crimes, show reductions; these gains can be achieved while simultaneously reducing racial disparities. These results suggest that while machine learning can be valuable, realizing this value requires integrating these tools into an economic framework: being clear about the link between predictions and decisions; specifying the scope of payoff functions; and constructing unbiased decision counterfactuals.},
   author = {Jon Kleinberg and Himabindu Lakkaraju and Jure Leskovec and Jens Ludwig and Sendhil Mullainathan},
   doi = {10.1093/QJE/QJX032},
   issn = {0033-5533},
   issue = {1},
   journal = {The Quarterly Journal of Economics},
   month = {2},
   pages = {237-293},
   publisher = {Oxford Academic},
   title = {Human Decisions and Machine Predictions},
   volume = {133},
   url = {https://dx-doi-org.lprx.bates.edu/10.1093/qje/qjx032},
   year = {2018},
}
@article{bertrand2004emily,
   abstract = {We study race in the labor market by sending fictitious resumes to help-wanted ads in Boston and Chicago newspapers. To manipulate perceived race, resumes are randomly assigned African-American- or White-sounding names. White names receive 50 percent more callbacks for interviews. Callbacks are also more responsive to resume quality for White names than for African-American ones. The racial gap is uniform across occupation, industry, and employer size. We also find little evidence that employers are inferring social class from the names. Differential treatment by race still appears to still be prominent in the U.S. labor market.},
   author = {Marianne Bertrand and Sendhil Mullainathan},
   doi = {10.1257/0002828042002561},
   issn = {0002-8282},
   issue = {4},
   journal = {American Economic Review},
   keywords = {Economics of Minorities, Races, Indigenous Peoples, and Immigrants,Non-labor Discrimination, Labor Discrimination},
   month = {8},
   pages = {991-1013},
   publisher = {American Economic Association},
   title = {Are Emily and Greg More Employable Than Lakisha and Jamal? A Field Experiment on Labor Market Discrimination},
   volume = {94},
   year = {2004},
}
@article{davis2017summer,
   author = {Jonathan M.V. Davis and Sara B. Heller},
   doi = {10.1257/AER.P20171000},
   issn = {0002-8282},
   issue = {5},
   journal = {American Economic Review},
   keywords = {Child Care,Children,Family Planning,Quantile Regressions, Fertility,Single Equation Models,Single Variables: Cross-Sectional Models,Spatial Models,Treatment Effect Models,Youth, Mobility, Unemployment, and Vacancies: Public Policy},
   month = {5},
   pages = {546-50},
   publisher = {American Economic Association},
   title = {Using Causal Forests to Predict Treatment Heterogeneity: An Application to Summer Jobs},
   volume = {107},
   year = {2017},
}
@article{wu2018gendered,
   abstract = {This paper examines the existence of an unwelcoming or stereotypical culture using evidence on how women and men are portrayed in anonymous discussions on the Economics Job Market Rumors forum (EJMR). I use a Lasso-Logistic model to measure gendered language in EJMR postings, identifying the words that are most strongly associated with discussions about one gender or the other. I find that the words most predictive of a post about a woman are typically about physical appearance or personal information, whereas those most predictive of a post about a man tend to focus on academic or professional characteristics.},
   author = {Alice H. Wu},
   doi = {10.1257/PANDP.20181101},
   issn = {2574-0768},
   journal = {AEA Papers and Proceedings},
   keywords = {Belief,Communication,Information and Knowledge,Learning,Market for Economists, Search,Non-labor Discrimination, Professional Labor Markets,Occupational Licensing,Role of Economics,Role of Economists,Unawareness, Economics of Gender},
   month = {5},
   pages = {175-79},
   publisher = {American Economic Association},
   title = {Gendered Language on the Economics Job Market Rumors Forum},
   volume = {108},
   year = {2018},
}
@article{chetty2014opportunity,
   abstract = {We use administrative records on the incomes of more than 40 million children and their parents to describe three features of intergenerational mobility in the United States. First, we characterize the joint distribution of parent and child income at the national level. The conditional expectation of child income given parent income is linear in percentile ranks. On average, a 10 percentile increase in parent income is associated with a 3.4 percentile increase in a child's income. Second, intergenerational mobility varies substantially across areas within the United States. For example, the probability that a child reaches the top quintile of the national income distribution starting from a family in the bottom quintile is 4.4% in Charlotte but 12.9% in San Jose. Third, we explore the factors correlated with upward mobility. High mobility areas have (i) less residential segregation, (ii) less income inequality, (iii) better primary schools, (iv) greater social capital, and (v) greater family stability. Although our descriptive analysis does not identify the causal mechanisms that determine upward mobility, the publicly available statistics on interge-nerational mobility developed here can facilitate research on such mechanisms.},
   author = {Raj Chetty and Nathaniel Hendren and Patrick Kline and Emmanuel Saez and David Autor and Gary Becker and David Card and David Dorn and John Friedman and James Heckman and Nathaniel Hilger and Richard Hornbeck and Lawrence Katz and Sara Lalumia and Adam Looney and Pablo Mitnik and Jonathan Parker and Laszlo Sandor and Gary Solon and Danny Yagan and Sarah Abraham and Alex Bell and Shelby Lin and Alex Olssen and Evan Storms and Michael Stepner and Wentao Xiong},
   doi = {10.1093/QJE/QJU022},
   issn = {0033-5533},
   issue = {4},
   journal = {The Quarterly Journal of Economics},
   month = {11},
   pages = {1553-1623},
   publisher = {Oxford Academic},
   title = {Where is the land of Opportunity? The Geography of Intergenerational Mobility in the United States},
   volume = {129},
   url = {https://dx.doi.org/10.1093/qje/qju022},
   year = {2014},
}
@article{chetty2018neighborhoods,
   abstract = {We show that the neighborhoods in which children grow up shape their earnings, college attendance rates, and fertility and marriage patterns by studying more than 7 million families who move across commuting zones and counties in the United States. Exploiting variation in the age of children when families move, we find that neighborhoods have significant childhood exposure effects: the outcomes of children whose families move to a better neighborhood-as measured by the outcomes of children already living there-improve linearly in proportion to the amount of time they spend growing up in that area, at a rate of approximately 4% per year of exposure. We distinguish the causal effects of neighborhoods from confounding factors by comparing the outcomes of siblings within families, studying moves triggered by displacement shocks, and exploiting sharp variation in predicted place effects across birth cohorts, genders, and quantiles to implement overidentification tests. The findings show that neighborhoods affect intergenerational mobility primarily through childhood exposure, helping reconcile conflicting results in the prior literature.},
   author = {Raj Chetty and Nathaniel Hendren},
   doi = {10.1093/QJE/QJY007},
   issn = {0033-5533},
   issue = {3},
   journal = {The Quarterly Journal of Economics},
   month = {8},
   pages = {1107-1162},
   publisher = {Oxford Academic},
   title = {The Impacts of Neighborhoods on Intergenerational Mobility I: Childhood Exposure Effects},
   volume = {133},
   url = {https://dx.doi.org/10.1093/qje/qjy007},
   year = {2018},
}
@article{metcalf2016humansubjects,
   abstract = {There are growing discontinuities between the research practices of data science and established tools of research ethics regulation. Some of the core commitments of existing research ethics regulations, such as the distinction between research and practice, cannot be cleanly exported from biomedical research to data science research. Such discontinuities have led some data science practitioners and researchers to move toward rejecting ethics regulations outright. These shifts occur at the same time as a proposal for major revisions to the Common Rule—the primary regulation governing human-subjects research in the USA—is under consideration for the first time in decades. We contextualize these revisions in long-running complaints about regulation of social science research and argue data science should be understood as continuous with social sciences in this regard. The proposed regulations are more flexible and scalable to the methods of non-biomedical research, yet problematically largely exclude data science methods from human-subjects regulation, particularly uses of public datasets. The ethical frameworks for Big Data research are highly contested and in flux, and the potential harms of data science research are unpredictable. We examine several contentious cases of research harms in data science, including the 2014 Facebook emotional contagion study and the 2016 use of geographical data techniques to identify the pseudonymous artist Banksy. To address disputes about application of human-subjects research ethics in data science, critical data studies should offer a historically nuanced theory of “data subjectivity” responsive to the epistemic methods, harms and benefits of data science and commerce.},
   author = {Jacob Metcalf and Kate Crawford},
   doi = {10.1177/2053951716650211/ASSET/IMAGES/LARGE/10.1177_2053951716650211-FIG1.JPEG},
   issn = {20539517},
   issue = {1},
   journal = {Big Data and Society},
   keywords = {Big Data,Common Rule,Data ethics,critical data studies,human subjects},
   month = {1},
   publisher = {SAGE Publications Ltd},
   title = {Where are human subjects in Big Data research? The emerging ethics divide},
   volume = {3},
   url = {https://journals.sagepub.com/doi/10.1177/2053951716650211},
   year = {2016},
}
@article{zook2017tensimplerules,
   author = {Matthew Zook and Solon Barocas and danah boyd and Kate Crawford and Emily Keller and Seeta Peña Gangadharan and Alyssa Goodman and Rachelle Hollander and Barbara A. Koenig and Jacob Metcalf and Arvind Narayanan and Alondra Nelson and Frank Pasquale},
   doi = {10.1371/JOURNAL.PCBI.1005399},
   isbn = {1111111111},
   issn = {1553-7358},
   issue = {3},
   journal = {PLOS Computational Biology},
   keywords = {Cloud computing,Data mining,Medicine and health sciences,Research and analysis methods,Research design,Research ethics,Scientists,Social media},
   month = {3},
   pages = {e1005399},
   pmid = {28358831},
   publisher = {Public Library of Science},
   title = {Ten simple rules for responsible big data research},
   volume = {13},
   url = {https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1005399},
   year = {2017},
}
@article{chetty2019privacy,
   abstract = {Building on insights from the differential privacy literature, we develop a simple noise-infusion method to reduce privacy loss when disclosing statistics such as OLS regression estimates based on small samples. Although our method does not offer a formal privacy guarantee, it outperforms widely used methods of disclosure limitation such as count-based cell suppression both in terms of privacy loss and statistical bias. We illustrate how the method can be implemented by discussing how it was used to release estimates of social mobility by census tract in the Opportunity Atlas. We provide a step-by-step guide and code to implement our approach.},
   author = {Raj Chetty and John N. Friedman},
   doi = {10.1257/PANDP.20191109},
   issn = {2574-0768},
   journal = {AEA Papers and Proceedings},
   keywords = {Data Access,Methodology for Collecting, Estimating, and Organizing Microeconomic Data},
   month = {5},
   pages = {414-20},
   publisher = {American Economic Association},
   title = {A Practical Method to Reduce Privacy Loss When Disclosing Statistics Based on Small Samples},
   volume = {109},
   year = {2019},
}
@article{ruggles2019differential,
   abstract = {JSTOR is a not-for-profit service that helps scholars, researchers, and students discover, use, and build upon a wide range of content in a trusted digital archive. We use information technology and tools to increase productivity and facilitate new forms of scholarship. For more information about JSTOR, please contact support@jstor.org.},
   author = {Steven Ruggles and Catherine Fitch and Diana Magnuson and Jonathan Schroeder},
   doi = {10.2307/26723980},
   isbn = {202322:00:41},
   pages = {403-408},
   title = {Differential Privacy and Census Data},
   volume = {109},
   year = {2019},
}
@inproceedings{hoffler2017replication,
   author = {Jan H. Höffler},
   doi = {10.1257/aer.p20171032},
   issn = {00028282},
   issue = {5},
   journal = {American Economic Review},
   month = {5},
   pages = {52-55},
   publisher = {American Economic Association},
   title = {Replication and economics journal policies},
   volume = {107},
   year = {2017},
}
@misc{gentzkowshapiro2014code,
   author = {Matthew M Gentzkow Jesse Shapiro and Chicago Booth and Matthew Gentzkow and Jesse M Shapiro},
   title = {Code and Data for the Social Sciences: A Practitioner's Guide},
   url = {http://faculty.chicagobooth.edu/matthew.gentzkow/research/CodeAndData.pdf,},
   year = {2014},
}
@article{chetty2022socialcapitali,
   abstract = {Social capital—the strength of an individual’s social network and community—has been identified as a potential determinant of outcomes ranging from education to health1–8. However, efforts to understand what types of social capital matter for these outcomes have been hindered by a lack of social network data. Here, in the first of a pair of papers9, we use data on 21 billion friendships from Facebook to study social capital. We measure and analyse three types of social capital by ZIP (postal) code in the United States: (1) connectedness between different types of people, such as those with low versus high socioeconomic status (SES); (2) social cohesion, such as the extent of cliques in friendship networks; and (3) civic engagement, such as rates of volunteering. These measures vary substantially across areas, but are not highly correlated with each other. We demonstrate the importance of distinguishing these forms of social capital by analysing their associations with economic mobility across areas. The share of high-SES friends among individuals with low SES—which we term economic connectedness—is among the strongest predictors of upward income mobility identified to date10,11. Other social capital measures are not strongly associated with economic mobility. If children with low-SES parents were to grow up in counties with economic connectedness comparable to that of the average child with high-SES parents, their incomes in adulthood would increase by 20% on average. Differences in economic connectedness can explain well-known relationships between upward income mobility and racial segregation, poverty rates, and inequality12–14. To support further research and policy interventions, we publicly release privacy-protected statistics on social capital by ZIP code at https://www.socialcapital.org . Analyses of data on 21 billion friendships from Facebook in the United States reveal associations between social capital and economic mobility.},
   author = {Raj Chetty and Matthew O. Jackson and Theresa Kuchler and Johannes Stroebel and Nathaniel Hendren and Robert B. Fluegge and Sara Gong and Federico Gonzalez and Armelle Grondin and Matthew Jacob and Drew Johnston and Martin Koenen and Eduardo Laguna-Muggenburg and Florian Mudekereza and Tom Rutter and Nicolaj Thor and Wilbur Townsend and Ruby Zhang and Mike Bailey and Pablo Barberá and Monica Bhole and Nils Wernerfelt},
   doi = {10.1038/s41586-022-04996-4},
   issn = {1476-4687},
   issue = {7921},
   journal = {Nature 2022 608:7921},
   keywords = {Economics,Sociology},
   month = {8},
   pages = {108-121},
   pmid = {35915342},
   publisher = {Nature Publishing Group},
   title = {Social capital I: measurement and associations with economic mobility},
   volume = {608},
   url = {https://www.nature.com/articles/s41586-022-04996-4},
   year = {2022},
}
@misc{chetty2020opportunity,
   author = {Raj Chetty and John Friedman and Nathaniel Hendren},
   title = {The Opportunity Atlas Mapping the Childhood Roots of Social Mobility},
   year = {2020},
}
@misc{chetty2022socialcapitaleconmob,
   author = {Raj Chetty and Matthew O Jackson and Theresa Kuchler and Johannes Stroebel and Abigail Hiller and Sarah Oppenheimer},
   title = {Social Capital and Economic Mobility},
   year = {2022},
}
@article{chetty2018opportunityatlas,
   abstract = {We construct a publicly available atlas of children's outcomes in adulthood by Census tract using anonymized longitudinal data covering nearly the entire U.S. population. For each tract, we estimate children's earnings distributions, incarceration rates, and other outcomes in adulthood by parental income, race, and gender. These estimates allow us to trace the roots of outcomes such as poverty and incarceration back to the neighborhoods in which children grew up. We find that children's outcomes vary sharply across nearby areas: for children of parents at the 25th percentile of the income distribution, the standard deviation of mean household income at age 35 is $5,000 across tracts within counties. We illustrate how these tract-level data can provide insight into how neighborhoods shape the development of human capital and support local economic policy using two applications. First, the estimates permit precise targeting of policies to improve economic opportunity by uncovering specific neighborhoods where certain subgroups of children grow up to have poor outcomes. Neighborhoods matter at a very granular level: conditional on characteristics such as poverty rates in a child's own Census tract, characteristics of tracts that are one mile away have little predictive power for a child's outcomes. Our historical estimates are informative predictors of outcomes even for children growing up today because neighborhood conditions are relatively stable over time. Second, we show that the observational estimates are highly predictive of neighborhoods' causal effects, based on a comparison to data from the Moving to Opportunity experiment and a quasi-experimental research design analyzing movers' outcomes. We then identify high-opportunity neighborhoods that are affordable to low-income families, providing an input into the design of affordable housing policies. Our measures of children's long-term outcomes are only weakly correlated with traditional proxies for local economic success such as rates of job growth, showing that the conditions that create greater upward mobility are not necessarily the same as those that lead to productive labor markets.},
   author = {Raj Chetty and John N Friedman and Nathaniel Hendren and Maggie R Jones and Sonya R Porter},
   city = {Cambridge, MA},
   doi = {10.3386/W25147},
   institution = {National Bureau of Economic Research},
   keywords = {John N. Friedman,Maggie R. Jones,Nathaniel Hendren,Raj Chetty,Sonya R. Porter},
   month = {10},
   title = {The Opportunity Atlas: Mapping the Childhood Roots of Social Mobility},
   url = {https://www.nber.org/papers/w25147},
   year = {2018},
}
@article{anderson2017replication,
   author = {Richard G. Anderson and Areerat Kichkha},
   doi = {10.1257/AER.P20171033},
   issn = {0002-8282},
   issue = {5},
   journal = {American Economic Review},
   keywords = {Sociology of Economics},
   month = {5},
   pages = {56-59},
   publisher = {American Economic Association},
   title = {Replication, Meta-analysis, and Research Synthesis in Economics},
   volume = {107},
   year = {2017},
}
@misc{cookson2023money,
   abstract = {Crowdfunding is an increasingly popular way to raise emergency funding after disasters. However, for victims of a recent major Colorado wildfire, we find that crowd-funding raised more support for high income and prime credit beneficiaries rather than helping the most vulnerable. Specifically, beneficiaries with income above $100,000 receive 40% more social network support on GoFundMe than beneficiaries with income below $50,000. High income households are also 13 percentage points more likely to have a crowdfunding campaign at all. The regressive nature of disaster crowdfunding is not due to less severe losses by financially vulnerable consumers, nor is it fully explained by the degree of online campaign sharing. Our findings highlight substantial disparities in social network insurance, which, as we show, likely exacerbate income inequalities in the recovery process. the seminar participants at the Property and Environment Research Center for helpful comments. We thank Gabe Bodner for explaining the steps in his disaster recovery process. All remaining errors are our own. This study received the approval of the University of Colorado's IRB (Protocol 22-0308). Experian has reviewed this draft for compliance with nondisclosure of individual personal credit information. † University of Colorado at Boulder.},
   author = {J Anthony Cookson and Colorado A Emily Gallagher and Philip Mulder and Shanna Cookson and Bryan Leonard and Nick Parker and Chris Stoddard and},
   keywords = {Charity,Inequality,Informal Insurance,Social Networks},
   title = {Money to Burn Wildfire Insurance via Social Networks *},
   url = {https://ssrn.com/abstract=4535190},
   year = {2023},
}
@article{edelman2017racial,
   abstract = {In an experiment on Airbnb, we find that applications from guests with distinctively African American names are 16 percent less likely to be accepted relative to identical guests with distinctively white names. Discrimination occurs among landlords of all sizes, including small landlords sharing the property and larger landlords with multiple properties. It is most pronounced among hosts who have never had an African American guest, suggesting only a subset of hosts discriminate. While rental markets have achieved significant reductions in discrimination in recent decades, our results suggest that Airbnb's current design choices facilitate discrimination and raise the possibility of erasing some of these civil rights gains.},
   author = {Benjamin Edelman and Michael Luca and Dan Svirsky},
   doi = {10.1257/APP.20160213},
   issn = {1945-7782},
   issue = {2},
   journal = {American Economic Journal: Applied Economics},
   keywords = {Field Experiments, Economics of Minorities, Races, Indigenous Peoples, and Immigrants,Gambling,Non-labor Discrimination, Sports,Recreation,Restaurants,Tourism},
   month = {4},
   pages = {1-22},
   publisher = {American Economic Association},
   title = {Racial Discrimination in the Sharing Economy: Evidence from a Field Experiment},
   volume = {9},
   url = {https://doi.org/10.1257/app.20160213},
   year = {2017},
}
@article{cook2021gender,
   abstract = {The growth of the "gig"economy generates worker flexibility that, some have speculated, will favour women. We explore this by examining labour supply choices and earnings among more than a million rideshare drivers on Uber in the U.S. We document a roughly 7% gender earnings gap amongst drivers. We show that this gap can be entirely attributed to three factors: Experience on the platform (learning-by-doing), preferences and constraints over where to work (driven largely by where drivers live and, to a lesser extent, safety), and preferences for driving speed. We do not find that men and women are differentially affected by a taste for specific hours, a return to within-week work intensity, or customer discrimination. Our results suggest that, in a "gig"economy setting with no gender discrimination and highly flexible labour markets, women's relatively high opportunity cost of non-paid-work time and gender-based differences in preferences and constraints can sustain a gender pay gap.},
   author = {Cody Cook and Rebecca Diamond and Jonathan V. Hall and John A. List and Paul Oyer},
   doi = {10.1093/RESTUD/RDAA081},
   issn = {0034-6527},
   issue = {5},
   journal = {The Review of Economic Studies},
   keywords = {Discrimination,Gender wage gap,Gig economy,J16,J31,J41,R41,Ridesharing,Transportation},
   month = {9},
   pages = {2210-2238},
   publisher = {Oxford Academic},
   title = {The Gender Earnings Gap in the Gig Economy: Evidence from over a Million Rideshare Drivers},
   volume = {88},
   url = {https://dx-doi-org.ezproxy.cul.columbia.edu/10.1093/restud/rdaa081},
   year = {2021},
}
@article{chen2020reservation,
   abstract = {Recent changes in labor arrangements have increased interest in estimating and understanding the value of job flexibility. We leverage a large natural field experiment at Uber to create exogenous variation in expected market wages across individuals and over time. Combining this experiment with high frequency panel data on wages and individual work decisions, we document how labor supply responds to exogenous changes in expected market wages in a setting with virtually no restrictions on driver labor allocation. We find that there is i) systematic heterogeneity in labor supply responses both across drivers and within a driver over time, ii) significant fixed costs of beginning a shift, and iii) high rider demand when it is costly for drivers to work. These three findings motivate a model of labor supply with heterogenous preferences over work schedules, adjustment costs, and statistical dependence between market wages and the costs of driving. We recover the labor supply elasticities and reservation wages of this dynamic labor supply model via a combination of experimental estimates and other data moments. We then perform counterfactual analyses that allow us to examine how preference heterogeneity and adjustment costs influence the responses of workers' to wage incentives as well as infer drivers' willingness to pay for the ability to customize and adjust their work schedule. We also show that a static approach to the driver's dynamic problem delivers materially different estimates of workers' labor supply elasticities and their value of job flexibility.},
   author = {Kuan-Ming A Chen Claire Ding John List Magne Mogstad and Oystein Daljord and Jonathan Hall and Vishal Kamat and Libby Mishkin and Jesse Shapiro and Winnie van Dijk and Kerry Smith and Kuan-Ming C Chen The Kenneth Griffin and Claire Ding and John A List and Magne Mogstad},
   city = {Cambridge, MA},
   doi = {10.3386/W27807},
   institution = {National Bureau of Economic Research},
   keywords = {Claire Ding,John A. List,Kuan-Ming Chen,Magne Mogstad},
   month = {9},
   title = {Reservation Wages and Workers’ Valuation of Job Flexibility: Evidence from a Natural Field Experiment},
   url = {https://www.nber.org/papers/w27807},
   year = {2020},
}
@article{halperin2022apologies,
   abstract = {We use a theory of apologies to design a nationwide field experiment involving 1.5 million Uber ridesharing consumers who experienced late rides. Several insights emerge. First, apologies are not a panacea - the efficacy of an apology and whether it may backfire depend on how the apology is made. Second, across treatments, money speaks louder than words - the best form of apology is to include a coupon for a future trip. Third, in some cases sending an apology is worse than sending nothing at all, particularly for repeated apologies and apologies that promise to do better. For firms, caveat venditor should be the rule when considering apologies.},
   author = {Basil Halperin and Benjamin Ho and John A. List and Ian Muir},
   doi = {10.1093/EJ/UEAB062},
   issn = {0013-0133},
   issue = {641},
   journal = {The Economic Journal},
   month = {1},
   pages = {273-298},
   publisher = {Oxford Academic},
   title = {Toward An Understanding of the Economics of Apologies: Evidence from a Large-Scale Natural Field Experiment},
   volume = {132},
   url = {https://dx-doi-org.ezproxy.cul.columbia.edu/10.1093/ej/ueab062},
   year = {2022},
}
@article{chandar2019socialpreference,
   abstract = {Even though social preferences affect nearly every facet of life, there exist many open questions on the economics of social preferences in markets. We leverage a unique opportunity to generate a large data set to inform the who's, what's, where's, and when's of social preferences through the lens of a nationwide tipping field experiment on the Uber platform. Our field experiment generates data from more than 40 million trips, allowing an exploration of social preferences in the ride sharing market using big data. Combining experimental and natural variation in the data, we are able to establish tipping facts as well as provide insights into the underlying motives for tipping. Interestingly, even though tips are made privately, and without external social benefits or pressure, more than 15% of trips are tipped. Yet, nearly 60% of people never tip, and only 1% of people always tip. Overall, the demand-side explains much more of the observed tipping variation than the supply-side.},
   author = {Bharat Chandar and Uri Gneezy and John A List and Ian Muir and Ian Muir Lyft},
   city = {Cambridge, MA},
   doi = {10.3386/W26380},
   institution = {National Bureau of Economic Research},
   keywords = {Bharat Chandar,Ian Muir,John A. List,Uri Gneezy},
   month = {10},
   title = {The Drivers of Social Preferences: Evidence from a Nationwide Tipping Field Experiment},
   url = {https://www.nber.org/papers/w26380},
   year = {2019},
}
@article{athey2017appliedeconometrics,
   abstract = {T he gold standard for drawing inferences about the effect of a policy is a randomized controlled experiment. However, in many cases, experiments remain difficult or impossible to implement, for financial, political, or ethical reasons, or because the population of interest is too small. For example, it would be unethical to prevent potential students from attending college in order to study the causal effect of college attendance on labor market experiences, and politically infea-sible to study the effect of the minimum wage by randomly assigning minimum wage policies to states. Thus, a large share of the empirical work in economics about policy questions relies on observational data-that is, data where policies were determined in a way other than through random assignment. Drawing inferences about the causal effect of a policy from observational data is quite challenging. To understand the challenges, consider the example of the minimum wage. A naive analysis of the observational data might compare the average employment level of states with a high minimum wage to that of states with a low minimum wage. This difference is surely not a credible estimate of the causal effect of a higher minimum wage, defined as the change in employment that would occur if the low-wage states raised their minimum wage. For example, it might be the case that states with higher costs of living, as well as more price-insensitive consumers, choose higher levels of the minimum wage},
   author = {Susan Athey and Guido W. Imbens},
   doi = {10.1257/JEP.31.2.3},
   issn = {0895-3309},
   issue = {2},
   journal = {Journal of Economic Perspectives},
   keywords = {Econometric Modeling: General,Econometrics},
   month = {3},
   pages = {3-32},
   publisher = {American Economic Association},
   title = {The State of Applied Econometrics: Causality and Policy Evaluation},
   volume = {31},
   url = {https://doi.org/10.1257/jep.31.2.3},
   year = {2017},
}
@article{ash2023textalgorithms,
   abstract = {This article provides an overview of the methods used for algorithmic text analysis in economics, with a focus on three key contributions. First, we introduce methods for representing documents as ...},
   author = {Elliott Ash and Stephen Hansen},
   doi = {10.1146/ANNUREV-ECONOMICS-082222-074352},
   issn = {1941-1383},
   issue = {1},
   journal = {https://doi.org/10.1146/annurev-economics-082222-074352},
   keywords = {C45,C55 Keywords text as data,JEL codes: C18,large language models,topic models,transformer models,word embeddings},
   month = {7},
   publisher = { Annual Reviews },
   title = {Text Algorithms in Economics},
   volume = {15},
   url = {https://www.annualreviews.org/doi/abs/10.1146/annurev-economics-082222-074352},
   year = {2023},
}
@article{davis2019segregation,
   abstract = {We provide measures of ethnic and racial segregation in urban consumption. Using Yelp reviews, we estimate how spatial and social frictions influence restaurant visits within New York City. Transit time plays a first-order role in consumption choices, so consumption segregation partly reflects residential segregation. Social frictions also affect restaurant choices: individuals are less likely to visit venues in neighborhoods demographically different from their own. While spatial and social frictions jointly produce significant levels of consumption segregation, we find that restaurant consumption is only about half as segregated as residences. Consumption segregation owes more to social than spatial frictions.},
   author = {Donald R. Davis and Jonathan I. Dingel and Joan Monras and Eduardo Morales},
   doi = {10.1086/701680/SUPPL_FILE/2016837DATA.ZIP},
   issn = {1537534X},
   issue = {4},
   journal = {Journal of Political Economy},
   month = {8},
   pages = {1684-1738},
   publisher = {University of Chicago Press},
   title = {How segregated is urban consumption?},
   volume = {127},
   url = {https://www.journals.uchicago.edu/doi/10.1086/701680},
   year = {2019},
}
@article{edelman2012internet,
   author = {Benjamin Edelman},
   doi = {10.1257/JEP.26.2.189},
   issn = {0895-3309},
   issue = {2},
   journal = {Journal of Economic Perspectives},
   keywords = {Computer Programs: General,Data Collection and Data Estimation Methodology},
   month = {3},
   pages = {189-206},
   title = {Using Internet Data for Economic Research},
   volume = {26},
   url = {http://dx.doi.org/10.1257/jep.26.2.189.},
   year = {2012},
}
@article{jensen2017introducing,
   abstract = {There is an insatiable demand in industry for data scientists, and graduate programs and certificates are gearing up to meet this demand. However, there is agreement in the industry that 80% of a data scientist's work consists of the transformation and profiling aspects of wrangling Big Data; work that may not require an advanced degree. In this paper we present hands-on exercises to introduce Big Data to undergraduate MIS students using the CoNVO Framework and Big Data tools to scope a data problem and then wrangle the data to answer questions using a real world dataset. This can provide undergraduates with a single course introduction to an important aspect of data science.},
   author = {Scott Jensen},
   doi = {10.24251/HICSS.2017.122},
   isbn = {978-0-9981331-0-2},
   issn = {15301605},
   journal = {Proceedings of the Annual Hawaii International Conference on System Sciences},
   keywords = {Big Data,Data Profiling,Data Science,Data Wrangling,Education},
   month = {1},
   pages = {1033-1042},
   publisher = {IEEE Computer Society},
   title = {Introducing Data Science to Undergraduates through Big Data: Answering Questions by Wrangling and Profiling a Yelp Dataset},
   volume = {2017-January},
   url = {http://hdl.handle.net/10125/41275},
   year = {2017},
}
@misc{naik2014streetscore,
   abstract = {Social science literature has shown a strong connection between the visual appearance of a city's neighborhoods and the behavior and health of its citizens. Yet, this research is limited by the lack of methods that can be used to quantify the appearance of streetscapes across cities or at high enough spatial resolutions. In this paper, we describe 'Streetscore', a scene understanding algorithm that predicts the perceived safety of a streetscape, using training data from an online survey with contributions from more than 7000 participants. We first study the predictive power of commonly used image features using support vector regression , finding that Geometric Texton and Color His-tograms along with GIST are the best performers when it comes to predict the perceived safety of a streetscape. Using Streetscore, we create high resolution maps of perceived safety for 21 cities in the Northeast and Midwest of the United States at a resolution of 200 images/square mile, scoring ∼1 million images from Google Streetview. These datasets should be useful for urban planners, economists and social scientists looking to explain the social and economic consequences of urban perception.},
   author = {Nikhil Naik and Jade Philipoom and Ramesh Raskar and Cesar Hidalgo},
   pages = {779-785},
   title = {Streetscore - Predicting the Perceived Safety of One Million Streetscapes},
   year = {2014},
}
@article{cavallo2016billion,
   author = {Alberto Cavallo and Roberto Rigobon},
   doi = {10.1257/JEP.30.2.151},
   issn = {0895-3309},
   issue = {2},
   journal = {Journal of Economic Perspectives},
   keywords = {Belief,Communication,Deflation,Inflation,Information and Knowledge,Large Data Sets: Modeling and Analysis, Search,Learning,Unawareness, Price Level},
   month = {3},
   pages = {151-78},
   publisher = {American Economic Association},
   title = {The Billion Prices Project: Using Online Prices for Measurement and Research},
   volume = {30},
   url = {http://dx.doi.org/10.1257/jep.30.2.151},
   year = {2016},
}
@article{glaeser2017local,
   abstract = {Can new data sources from online platforms help to measure local economic activity? Government datasets from agencies such as the U.S. Census Bureau provide the standard measures of local economic activity at the local level. However, these statistics typically appear only after multi-year lags, and the public-facing versions are aggregated to the county or ZIP code level. In contrast, crowdsourced data from online platforms such as Yelp are often contemporaneous and geographically finer than official government statistics. In this paper, we present evidence that Yelp data can complement government surveys by measuring economic activity in close to real time, at a granular level, and at almost any geographic scale. Changes in the number of businesses and restaurants reviewed on Yelp can predict changes in the number of overall establishments and restaurants in County Business Patterns. An algorithm using contemporaneous and lagged Yelp data can explain 29.2 percent of the residual variance after accounting for lagged CBP data, in a testing sample not used to generate the algorithm. The algorithm is more accurate for denser, wealthier, and more educated ZIP codes.},
   author = {Edward L. Glaeser and Hyunjin Kim and Michael Luca},
   city = {Cambridge, MA},
   doi = {10.3386/W24010},
   institution = {National Bureau of Economic Research},
   keywords = {Edward L. Glaeser,Hyunjin Kim,Michael Luca},
   month = {11},
   title = {Nowcasting the Local Economy: Using Yelp Data to Measure Economic Activity},
   url = {https://www.nber.org/papers/w24010},
   year = {2017},
}
@article{glaeser2018gentrification,
   abstract = {Data from digital platforms have the potential to improve our understanding of gentrification, both by predicting gentrification and by characterizing the local economy of gentrifying neighborhoods. To explore, we identify gentrifying neighborhoods using government data, and then use Yelp data to analyze local business activity. We find that gentrifying neighborhoods tend to have growing numbers of local groceries, cafes, restaurants, and bars, with little evidence of crowd-out of other types of businesses. Moreover, local economic activity, as measured by Yelp data, is a leading indicator for housing price changes and can help to predict which neighborhoods are gentrifying.},
   author = {Edward L. Glaeser and Hyunjin Kim and Michael Luca},
   doi = {10.1257/PANDP.20181034},
   issn = {2574-0768},
   journal = {AEA Papers and Proceedings},
   keywords = {Forecasting Models,Population,Regional Labor Markets,Simulation Methods, Survey Methods},
   month = {5},
   pages = {77-82},
   publisher = {American Economic Association},
   title = {Nowcasting Gentrification: Using Yelp Data to Quantify Neighborhood Change},
   volume = {108},
   url = {https://doi.org/10.1257/pandp.20181034},
   year = {2018},
}
@article{huntingtonklen2021influence,
   abstract = {Researchers make hundreds of decisions about data collection, preparation, and analysis in their research. We use a many-analysts approach to measure the extent and impact of these decisions. Two published causal empirical results are replicated by seven replicators each. We find large differences in data preparation and analysis decisions, many of which would not likely be reported in a publication. No two replicators reported the same sample size. Statistical significance varied across replications, and for one of the studies the effect's sign varied as well. The standard deviation of estimates across replications was 3–4 times the mean reported standard error.},
   author = {Nick Huntington-Klein and Andreu Arenas and Emily Beam and Marco Bertoni and Jeffrey R. Bloem and Pralhad Burli and Naibin Chen and Paul Grieco and Godwin Ekpe and Todd Pugatch and Martin Saavedra and Yaniv Stopnitzky},
   doi = {10.1111/ECIN.12992},
   issn = {1465-7295},
   issue = {3},
   journal = {Economic Inquiry},
   keywords = {metascience,replication,research},
   month = {7},
   pages = {944-960},
   publisher = {John Wiley & Sons, Ltd},
   title = {The influence of hidden researcher decisions in applied microeconomics},
   volume = {59},
   url = {https://onlinelibrary.wiley.com/doi/full/10.1111/ecin.12992 https://onlinelibrary.wiley.com/doi/abs/10.1111/ecin.12992 https://onlinelibrary.wiley.com/doi/10.1111/ecin.12992},
   year = {2021},
}
@unpublished{chetty2023diversifying,
   abstract = {Leadership positions in the U.S. are disproportionately held by graduates of a few highly selective private colleges. Could such colleges-which currently have many more students from high-income families than low-income families-increase the socioeconomic diversity of America's leaders by changing their admissions policies? We use anonymized admissions data from several private and public colleges linked to income tax records and SAT and ACT test scores to study this question. Children from families in the top 1% are more than twice as likely to attend an Ivy-Plus college (Ivy League, Stanford, MIT, Duke, and Chicago) as those from middle-class families with comparable SAT/ACT scores. Two-thirds of this gap is due to higher admissions rates for students with comparable test scores from high-income families; the remaining third is due to differences in rates of application and matriculation. In contrast, children from high-income families have no admissions advantage at flagship public colleges. The high-income admissions advantage at private colleges is driven by three factors: (1) preferences for children of alumni, (2) weight placed on non-academic credentials, which tend to be stronger for students applying from private high schools that have affluent student bodies, and (3) recruitment of athletes, who tend to come from higher-income families. Using a new research design that isolates idiosyncratic variation in admissions decisions for waitlisted applicants, we show that attending an Ivy-Plus college instead of the average highly selective public flagship institution increases students' chances of reaching the top 1% of the earnings distribution by 60%, nearly doubles their chances of attending an elite graduate school, and triples their chances of working at a prestigious firm. Ivy-Plus colleges have much smaller causal effects on average earnings, reconciling our findings with prior work that found smaller causal effects using variation in matriculation decisions conditional on admission. Adjusting for the value-added of the colleges that students attend, the three key factors that give children from high-income families an admissions advantage are uncorrelated or negatively correlated with post-college outcomes, whereas SAT/ACT scores and academic credentials are highly predictive of post-college success. We conclude that highly selective private colleges currently amplify the persistence of privilege across generations, but could diversify the socioeconomic backgrounds of America's leaders by changing their admissions practices.},
   author = {Raj Chetty and David J. Deming and John N. Friedman},
   city = {Cambridge, MA},
   doi = {10.3386/W31492},
   institution = {National Bureau of Economic Research},
   journal = {National Bureau of Economic Research},
   keywords = {David J. Deming,John N. Friedman,Raj Chetty},
   month = {7},
   title = {Diversifying Society’s Leaders? The Causal Effects of Admission to Highly Selective Private Colleges},
   url = {https://www.nber.org/papers/w31492},
   year = {2023},
}
@article{floridi2016ethics,
   abstract = {This theme issue has the founding ambition of landscaping data ethics as a new branch of ethics that studies and evaluates moral problems related to data (including generation, recording, curation,...},
   author = {Luciano Floridi and Mariarosaria Taddeo},
   doi = {10.1098/RSTA.2016.0360},
   issn = {1364503X},
   issue = {2083},
   journal = {Philosophical Transactions of the Royal Society A: Mathematical, Physical and Engineering Sciences},
   keywords = {data ethics,data science,ethics of algorithms,ethics of data,ethics of practices,levels of abstraction},
   month = {12},
   pages = {20160360},
   pmid = {28336805},
   publisher = {
The Royal Society
},
   title = {What is data ethics?},
   volume = {374},
   url = {https://royalsocietypublishing.org/doi/10.1098/rsta.2016.0360},
   year = {2016},
}
@article{einav2014ageofbigdata,
   abstract = {The quality and quantity of data on economic activity are expanding rapidly. Empirical research increasingly relies on newly available large-scale administrative data or private sector data that often is obtained through collaboration with private firms. Here we highlight some challenges in accessing and using these new data. We also discuss how new data sets may change the statistical methods used by economists and the types of questions posed in empirical research. T he expansion of data being collected on social and economic activity is likely to have profound effects on economic research. In this Review, we describe how newly available public and private sector data sets are being employed in economics. We also discuss how statistical methods in economics may adapt to take advantage of large-scale granu-lar data, as well as some of the challenges and opportunities for future empirical research. After providing some brief background in the next section, we divide the Review into three parts. We first discuss the shift from relatively small-sample government surveys to administrative data with universal or near-universal population coverage. These data have been used in Europe for some time but are just starting to be explored in the United States. We explain the transformative power of these data to shed light on variation across subpopulations, construct consistent long-run statistical indices, generate new quasi-experimental research designs, and track diverse outcomes from natural and controlled experiments. The second part of the Review describes the marked expansion of private sector data on economic activity. We outline the potential of these data in creating aggregate economic statistics and some nascent attempts to do this. We then discuss the rise of collaborations between academics and data-rich companies. These relationships have some trade-offs in terms of maintaining data confidentiality and working with samples that have been collected for business rather than research purposes. But as we illustrate with examples from recent work, they also provide researchers with a look inside the "black box" of firms and markets and create new opportunities to conduct and evaluate randomized experiments. The third part of this Review addresses statistical methods and the role of economic theory in the analysis of large-scale data sets. Today, economists routinely analyze large data sets with the same econometric methods used 15 or 20 years ago. We contrast these methods to some of the newer data mining approaches that have become popular in statistics and computer science. Economists, who tend to place a high premium on statistical inference and the identification of causal effects, have been skeptical about these methods, which put more emphasis on predic-tive fit and handling model uncertainty and on identifying low-dimensional structure in high-dimensional data. We argue that there are considerable gains from trade. We also stress the usefulness of economic theory in helping to organize complex and unstructured data. We conclude by discussing a few challenges in making use of new data opportunities, in particular the need to incorporate data management skills into economics training, and the difficulties of data access and research transparency in the presence of privacy and confidentiality concerns. The rise of empirical economics Hamermesh (1) recently reviewed publications from 1963 to 2011 in top economics journals. Until the mid-1980s, the majority of papers were theoretical; the remainder relied mainly on "ready-made" data from government statistics or surveys. Since then, the share of empirical papers in top journals has climbed to more than 70%, and a substantial majority of these papers use data that have been assembled or obtained by the authors or generated through a controlled experiment. This shift mirrors the expansion of available data. Even 15 or 20 years ago, interesting and unstudied data sets were a scarce resource. Gathering data on a specific industry could involve hunting through the library or manually extracting statistics from trade publications. Collaborations with companies were unusual, as were experiments , both in laboratory settings and in the field. Nowadays the situation is very different along all of these dimensions. Apart from simply having more observations and more recorded data in each observation, several features differentiate modern data sets from many used in earlier research. The first feature is that data are now often available in real time. Government surveys and statistics are released with a lag of months or years. Of course, many research questions are naturally retrospective, and it is more important for data to be detailed and accurate rather than available immediately. However, administrative and private data that are continuously updated have great value for helping to guide economic policy. Below, we discuss some early attempts to use Internet data to make real-time forecasts of inflation, retail sales, and labor market activity and to create new tracking measures of the economy. The second feature is that data are available on previously unmeasured activities. Much of the data now being recorded is on activities that were previously difficult to quantify: personal communications, social networks, search and information gathering, and geolocation data. These data may open the door to studying issues that economists have long viewed as important but did not have good ways to study empirically, such as the role of social connections and geographic proximity in shaping preferences, the transmission of information, consumer purchasing behavior, productivity, and job search. Finally, data come with less structure. Economists are used to working with "rectangular" data, with N observations and K << N variables per observation and a relatively simple dependence structure between the observations. New data sets often have higher dimensionality and less-clear structure. For example, Internet browsing histories contain a great deal of information about a person's interests and beliefs and how they evolve over time. But how can one extract this infor-mation? The data record a sequence of events that can be organized in an enormous number of ways, which may or may not be clearly linked and from which an almost unlimited number of variables can be created. Figuring out how to organize and reduce the dimensionality of large-scale, unstructured data is becoming a crucial challenge in empirical economic research. Public sector data: Administrative records In the course of administering the tax system, social programs, and regulation, the federal government collects highly detailed data on individuals and corporations. The same is true of state and local governments, albeit with less uniformity , in areas such as education, social insurance, and local government spending. As electronic versions of these data become available, they increasingly are the resource of choice for economists who work in fields such as labor economics, public finance, health, and education. Administrative data offer several advantages over traditional survey data. Workhorse surveys-such as the Survey of Consumer Finances, the Current Population Survey, the Survey of Income and Program Participation, and the Panel Study on Income Dynamics-can suffer from substantial missing data issues, and the sample size may be limited in ways that preclude natural quasi-experimental research designs (2). The rich microlevel administrative data sets maintained by, among others, the Social Security Administration , the Internal Revenue Service, and the Centers for Medicare and Medicaid, often have},
   author = {Liran Einav and Jonathan Levin},
   issue = {6210},
   journal = {Science},
   pages = {1-7},
   title = {Economics in the age of big data},
   volume = {346},
   url = {https://www.science.org},
   year = {2014},
}
@article{varian2014bigdata,
   author = {Hal R. Varian},
   doi = {10.1257/JEP.28.2.3},
   issn = {0895-3309},
   issue = {2},
   journal = {Journal of Economic Perspectives},
   keywords = {Modeling with Large Data Sets},
   pages = {3-28},
   publisher = {American Economic Association},
   title = {Big Data: New Tricks for Econometrics},
   volume = {28},
   url = {http://dx.doi.org/10.1257/jep.28.2.3},
   year = {2014},
}
@article{giannone2021predictions,
   abstract = {We compare sparse and dense representations of predictive models in macroeconomics, microeconomics, and finance. To deal with a large number of possible predictors, we specify a prior that allows for both variable selection and shrinkage. The posterior distribution does not typically concentrate on a single sparse model, but on a wide set of models that often include many predictors.},
   author = {Domenico Giannone and Michele Lenza and Giorgio E. Primiceri},
   doi = {10.3982/ECTA17842},
   issn = {1468-0262},
   issue = {5},
   journal = {Econometrica},
   keywords = {Variable selection,high dimensional data,model uncertainty,shrinkage},
   month = {9},
   pages = {2409-2437},
   publisher = {John Wiley & Sons, Ltd},
   title = {Economic Predictions With Big Data: The Illusion of Sparsity},
   volume = {89},
   url = {https://onlinelibrary-wiley-com.ezproxy.cul.columbia.edu/doi/full/10.3982/ECTA17842 https://onlinelibrary-wiley-com.ezproxy.cul.columbia.edu/doi/abs/10.3982/ECTA17842 https://onlinelibrary-wiley-com.ezproxy.cul.columbia.edu/doi/10.3982/ECTA17842},
   year = {2021},
}
@article{Glaeser2018,
   abstract = {New, “big data” sources allow measurement of city characteristics and outcome variables at higher collection frequencies and more granular geographic scales than ever before. However, big data will not solve large urban social science questions on its own. Big urban data has the most value for the study of cities when it allows measurement of the previously opaque, or when it can be coupled with exogenous shocks to people or place. We describe a number of new urban data sources and illustrate how they can be used to improve the study and function of cities. We first show how Google Street View images can be used to predict income in New York City, suggesting that similar imagery data can be used to map wealth and poverty in previously unmeasured areas of the developing world. We then discuss how survey techniques can be improved to better measure willingness to pay for urban amenities. Finally, we explain how Internet data is being used to improve the quality of city services. (JEL R1, C8, C18).},
   author = {Edward L. Glaeser and Scott Duke Kominers and Michael Luca and Nikhil Naik},
   doi = {10.1111/ecin.12364},
   issn = {14657295},
   issue = {1},
   journal = {Economic Inquiry},
   month = {1},
   pages = {114-137},
   publisher = {Blackwell Publishing Inc.},
   title = {Big Data and Big Cities: The Promises and Limitations of Improved Measures of Urban Life},
   volume = {56},
   year = {2018},
}
@article{athey2019machine,
   author = {Susan Athey and Guido W. Imbens},
   journal = {Annual Review of Economics},
   title = {Machine Learning Methods That Economists Should Know About},
   volume = {11},
   year = {2019},
}
@article{gentzkow2019text,
   author = {Matthew Gentzkow and Bryan Kelly and Matt Taddy},
   doi = {10.2307/26787457},
   issue = {3},
   journal = {Source: Journal of Economic Literature},
   pages = {535-574},
   title = {Text as Data},
   volume = {57},
   year = {2019},
}
@article{chetty2014impactii,
   abstract = {Are teachers' impacts on students' test scores (value-added) a good measure of their quality? This question has sparked debate partly because of a lack of evidence on whether high value-added (VA) teachers improve students' long-term outcomes. Using school district and tax records for more than one million children, we find that students assigned to high-VA teachers are more likely to attend college, earn higher salaries, and are less likely to have children as teenagers. Replacing a teacher whose VA is in the bottom 5 percent with an average teacher would increase the present value of students' lifetime income by approximately $250,000 per classroom.},
   author = {Raj Chetty and John N. Friedman and Jonah E. Rockoff},
   doi = {10.1257/aer.104.9.2633},
   issn = {00028282},
   journal = {American Economic Review},
   title = {Measuring the impacts of teachers II: Teacher value-added and student outcomes in adulthood},
   year = {2014},
}
@article{chetty2014impacti,
   abstract = {Are teachers' impacts on students' test scores (value-added) a good measure of their quality? One reason this question has sparked debate is disagreement about whether value-added (VA) measures provide unbiased estimates of teachers' causal impacts on student achievement. We test for bias in VA using previously unobserved parent characteristics and a quasi-experimental design based on changes in teaching staff. Using school district and tax records for more than one million children, we find that VA models which control for a student's prior test scores provide unbiased forecasts of teachers' impacts on student achievement.},
   author = {Raj Chetty and John N. Friedman and Jonah E. Rockoff},
   doi = {10.1257/aer.104.9.2593},
   issn = {00028282},
   journal = {American Economic Review},
   title = {Measuring the impacts of teachers I: Evaluating bias in teacher value-added estimates},
   year = {2014},
}
